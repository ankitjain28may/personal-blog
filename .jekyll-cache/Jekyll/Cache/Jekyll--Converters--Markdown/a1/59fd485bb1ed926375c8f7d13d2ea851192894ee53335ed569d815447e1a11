I"Ö<p>Week 5 of the GSoC coding period is completed successfully and with this, the first evaluation is also completed successfully. <a href="https://summerofcode.withgoogle.com/">GSoC</a>¬†(Google Summer of Code) is a global program focused on bringing more student developers into open source software development. Students work with an open source organization on a 3-month programming project during their break from school.</p>

<h3 id="project-abstract"><strong>Project Abstract</strong></h3>

<p>I am working on ‚Äú<strong>Developing a ‚Äú Product Advertising API ‚Äù module for Drupal 8</strong>‚Äù -¬†<a href="https://groups.drupal.org/node/518074">#7</a>. The ‚ÄúProduct Advertising API‚Äù which is renamed to ‚Äú<a href="https://www.drupal.org/project/affiliates_connect">Affiliates Connect</a>‚Äù module provides an interface to easily integrate with affiliate APIs or product advertising APIs provided by different e-commerce platforms like Flipkart, Amazon, eBay etc to fetch data easily from their database and import them into Drupal to monetize your website by advertising their products. If the e-commerce platforms don‚Äôt provide affiliate APIs, we will scrape the data from such platforms.</p>

<h3 id="progress"><strong>Progress</strong></h3>

<p>Some of the tasks accomplished in this week are-</p>

<ul>
  <li>Native API for affiliates_connect_flipkart module is integrated but it needs to be reviewed. As we decided to create separate module for Flipkart and other E-commerce similar to Social Auth. I have used Batch API for fetching products data from Flipkart Affiliate API so that we can fetch all the products without facing any error else our request will get end due to exceeding max_execution_time in php. Its code is on Github, Link to the repo - <a href="https://github.com/ankitjain28may/scraping-using-node/tree/master/Flipkart/affiliates_connect_flipkart">affiliates_connect_flipkart</a></li>
</ul>

<p><img src="/mediumish-theme-jekyll/assets/drupal/inline-images/Screenshot-2018-6-19%20Importing%20products%20from%2058%20categories%20Drupal.png" alt="Flipkart module" /></p>

<ul>
  <li>
    <p>Fetcher for the Scraper API is complete. I have created two fetchers (StaticFetcher and DynamicFetcher). StaticFetcher and DynamicFetcher leveraging the power of Feeds module and save the content of URL in a temporary file which is further passed to the parser for parsing. This is under review and Link to the issue - <a href="https://www.drupal.org/project/affiliates_connect/issues/2979094">#2979094</a></p>
  </li>
  <li>
    <p>Fetcher will need the Nodejs Server as it is using Request lib for fetching the content from the URL and nightmare for websites which use JS to load DOM. It code is under this repo - <a href="https://github.com/ankitjain28may/scraping-using-node/tree/master/Scraper">Scraper</a></p>
  </li>
</ul>

<h3 id="week-6---goals"><strong>Week 6 - Goals</strong></h3>

<ul>
  <li>
    <p>In this week, I will work on adding the pagination to the fetcher so that we can crawl and scrape multiple pages of the same category/type. In this way, we need not to create feeds for every single page which is impossible for anyone to do. Link to the issue - <a href="https://www.drupal.org/project/affiliates_connect/issues/2980450">#2980450</a></p>
  </li>
  <li>
    <p>I will also add functional tests for the checking whether the plugin (affiliates_connect_amazon) is instantiated properly after installing and other tests for the configuration form. Link to the issue - <a href="https://www.drupal.org/project/affiliates_connect/issues/2979801">#2979801</a></p>
  </li>
  <li>
    <p>I will try to complete the parser this week so that we can make good progress and complete the scraper part of the module.</p>
  </li>
</ul>

<h3 id="difficulties"><strong>Difficulties</strong></h3>

<ul>
  <li>
    <p>Paginating every page is difficult but as feeds module is using Batch API and as in this issue - <a href="https://www.drupal.org/project/feeds/issues/2979249">#2979249</a> it is mentioned - ‚ÄúFeeds asks the fetcher if there is something to fetch. With the returned result from the fetcher, it asks the parser to parse it. This is results in a number of items and for each item, Feeds asks the processor to process it. After all, items are processed, Feeds asks the parser again if it has more to parse. If so, this would result in a number of items again where each of them is passed to the processor. If the parser has nothing more to parse, Feeds will ask the fetcher again if it has more to fetch.‚Äù So It‚Äôs not going to be difficult.</p>
  </li>
  <li>
    <p>Parser to parse the result of the fetcher and pass the result to the processor will be challenging.</p>
  </li>
</ul>
:ET